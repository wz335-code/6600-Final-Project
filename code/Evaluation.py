import pandas as pd
import torch
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm

# ========================================================
# 1. ç»ˆæç¨³å®šé…ç½® (Python 3.10 + CPU + Verified Model)
# ========================================================

# ç¦ç”¨å¹¶è¡Œï¼Œé˜²æ­¢æ­»é”
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# å¼ºåˆ¶ CPU (åœ¨ Python 3.10 ä¸‹éå¸¸ç¨³å®š)
DEVICE = "cpu"
print(f"ğŸ›¡ï¸ Running on device: {DEVICE} (Stable Mode)")

# âœ… è¿™æ˜¯ä¸€ä¸ªçœŸå®å­˜åœ¨ã€æ¶æ„æˆç†Ÿçš„ CEFR æ¨¡å‹
CEFR_MODEL_NAME = "AbdulSami/bert-base-cased-cefr"

# ========================================================
# 2. æ ¸å¿ƒé€»è¾‘
# ========================================================

def load_model():
    print(f"Loading model: {CEFR_MODEL_NAME}...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(CEFR_MODEL_NAME)
        model = AutoModelForSequenceClassification.from_pretrained(CEFR_MODEL_NAME)
        model.to(DEVICE)
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼(Python 3.10 ç¯å¢ƒéªŒè¯é€šè¿‡)")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def predict_batch(texts, tok, model, batch_size=16):
    labels = []
    
    # è‡ªåŠ¨è¯»å–æ¨¡å‹çš„æ ‡ç­¾æ˜ å°„ {0: 'A1', 1: 'A2'...}
    id2label = model.config.id2label
    
    model.eval()
    with torch.no_grad():
        for start in tqdm(range(0, len(texts), batch_size), desc="Evaluating"):
            batch = texts[start:start + batch_size]
            if not batch: continue

            enc = tok(
                batch,
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt",
            ).to(DEVICE)

            logits = model(**enc).logits
            idxs = logits.argmax(dim=-1)

            for i in range(len(batch)):
                idx = int(idxs[i].item())
                # ç›´æ¥ä»æ¨¡å‹é…ç½®é‡Œæ‹¿æ ‡ç­¾ï¼Œä¸å†æ€•é¡ºåºæé”™
                label = id2label.get(idx, "Unknown")
                labels.append(label)
                
    return labels

# ========================================================
# 3. æ‰§è¡Œæ¸…æ´—
# ========================================================

if __name__ == "__main__":
    tokenizer, model = load_model()

    if model:
        # ğŸ”´ è®°å¾—æŠŠè¿™é‡Œæ”¹æˆä½ å…·ä½“è¦æ¸…æ´—çš„æ–‡ä»¶å
        input_file = "cambridge_simplified_only.csv"
        
        if os.path.exists(input_file):
            df = pd.read_csv(input_file)
            print(f"åŸå§‹æ•°æ®: {len(df)} è¡Œ")
            
            # æ”¶é›†ä»»åŠ¡
            tasks = []
            target_cols = [f"text_{lvl}" for lvl in ["A1", "A2", "B1", "B2", "C1", "C2"]]
            
            for idx, row in df.iterrows():
                for col in target_cols:
                    if col in df.columns and pd.notna(row[col]):
                        tasks.append((idx, col, str(row[col])))

            if tasks:
                print(f"å¼€å§‹éªŒè¯ {len(tasks)} ä¸ªç‰‡æ®µ...")
                all_texts = [t[2] for t in tasks]
                preds = predict_batch(all_texts, tokenizer, model, batch_size=16)

                kept, dropped = 0, 0
                for i, (row_idx, col, text) in enumerate(tasks):
                    target_lvl = col.split("_")[1] # text_A1 -> A1
                    pred_lvl = preds[i]
                    
                    # ç®€å•æ¸…æ´—ä¸€ä¸‹æ ‡ç­¾ (æœ‰äº›æ¨¡å‹è¾“å‡ºå¸¦æœ‰ç©ºæ ¼)
                    if pred_lvl.strip() == target_lvl:
                        kept += 1
                    else:
                        df.at[row_idx, col] = None
                        dropped += 1
                
                # ä¿å­˜
                df.dropna(subset=target_cols, how='all').to_csv("cambridge_verified_final.csv", index=False)
                print(f"\nğŸ‰ æˆåŠŸï¼ä¿ç•™: {kept} | åˆ é™¤: {dropped}")
                print("ç»“æœå·²ä¿å­˜è‡³: cambridge_verified_final.csv")
            else:
                print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦éªŒè¯çš„æ•°æ®åˆ—ï¼")
        else:
            print(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {input_file}")