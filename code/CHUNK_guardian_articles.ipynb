{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cdfa634-5312-4c7a-a584-171dc6f375cb",
   "metadata": {},
   "source": [
    "CHUNK: guardian_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291a8ba8-8cb0-40f0-bb61-de1805dc4a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wenhaozhou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/wenhaozhou/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers nltk pandas\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab') # !!! punkt_tab\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be01a45-f57b-47ba-9b20-8ea1f065c14b",
   "metadata": {},
   "source": [
    "# Fix and upgrade：\n",
    "1. If we use \"chunk_size = 512\", That means we only have 512 single letters, rather than 512 words. So we should use \"tokenizer.encode\" to generate the true 512 tokens.\n",
    "2. \"RecursiveCharacterTextSplitter\" might cut a sentence in the middle of nowhere. We can use \"nltk.sent_tokenize\" to ensure that we recognize the whole sentence. Therefore, we can simply save more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52aa1b85-8e00-4eab-b005-4176c113ae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tokens amount: 360\n",
      "cutting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 360/360 [00:00<00:00, 614.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ mission accomplished！\n",
      "count our chunks: 1138\n",
      "save as: guardian_articles_chunked.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>0</td>\n",
       "      <td>Celso Amorim joined Brazil’s foreign service n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0_1</td>\n",
       "      <td>1</td>\n",
       "      <td>“Things which have been building up for years ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0_2</td>\n",
       "      <td>2</td>\n",
       "      <td>Mass demonstrations rocked Mexico in 2014 afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0_3</td>\n",
       "      <td>3</td>\n",
       "      <td>“There hasn’t yet been a popular reaction [aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_0</td>\n",
       "      <td>0</td>\n",
       "      <td>Protesters in Iraq have dealt a symbolic blow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index chunk_id  chunk_index  \\\n",
       "0               0      0_0            0   \n",
       "1               0      0_1            1   \n",
       "2               0      0_2            2   \n",
       "3               0      0_3            3   \n",
       "4               1      1_0            0   \n",
       "\n",
       "                                                text  \n",
       "0  Celso Amorim joined Brazil’s foreign service n...  \n",
       "1  “Things which have been building up for years ...  \n",
       "2  Mass demonstrations rocked Mexico in 2014 afte...  \n",
       "3  “There hasn’t yet been a popular reaction [aga...  \n",
       "4  Protesters in Iraq have dealt a symbolic blow ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. import\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. get ready\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Flan-T5 Tokenizer\n",
    "model_checkpoint = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Split long texts into segments of no more than 512 tokens, keeping sentences intact.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    # A. split sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # B. calculate the token numbers\n",
    "        token_count = len(tokenizer.encode(\" \" + sentence, add_special_tokens=False))\n",
    "        \n",
    "        # C. Judge: is it longer than our threshold 512 ?\n",
    "        if current_length + token_count <= max_length:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += token_count\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            \n",
    "            # open our new chunks\n",
    "            current_chunk = [sentence]\n",
    "            current_length = token_count\n",
    "            \n",
    "    # save chunks\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# 2. read our guardian_articles\n",
    "input_file = \"guardian_articles.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "print(f\"original tokens amount: {len(df)}\")\n",
    "\n",
    "# 3. start splitting\n",
    "new_rows = []\n",
    "\n",
    "print(\"cutting...\")\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    full_text = row['text'] if pd.notna(row['text']) else \"\"\n",
    "    \n",
    "    if not full_text:\n",
    "        continue\n",
    "        \n",
    "    # make chunks\n",
    "    chunked_texts = chunk_text(full_text, max_length=512)\n",
    "    \n",
    "    # save our results\n",
    "    for i, chunk in enumerate(chunked_texts):\n",
    "        new_rows.append({\n",
    "            \"original_index\": index,\n",
    "            \"chunk_id\": f\"{index}_{i}\",\n",
    "            \"chunk_index\": i, # chunk sequence\n",
    "            \"text\": chunk # text for each chunks\n",
    "        })\n",
    "\n",
    "# 4. generate our new file\n",
    "df_chunked = pd.DataFrame(new_rows)\n",
    "output_file = \"guardian_articles_chunked.csv\"\n",
    "df_chunked.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✅ mission accomplished！\")\n",
    "print(f\"count our chunks: {len(df_chunked)}\")\n",
    "print(f\"save as: {output_file}\")\n",
    "\n",
    "# small check\n",
    "df_chunked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d73790-4695-4946-af55-83ad34f8a1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
