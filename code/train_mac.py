import os
import torch
import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import Optional

from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
from peft import LoraConfig, get_peft_model, TaskType

# =====================================
# 1. Mac M4 ä¸“å±é…ç½®
# =====================================
# ç¦ç”¨å¹¶è¡Œï¼Œé˜²æ­¢ Mac å‡ºç° "Leaked semaphore" æŠ¥é”™
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ£€æŸ¥ MPS åŠ é€Ÿ
if torch.backends.mps.is_available():
    DEVICE = "mps"
    print("ğŸš€ æ­£åœ¨ä½¿ç”¨ Apple MPS (GPU) åŠ é€Ÿï¼")
else:
    DEVICE = "cpu"
    print("âš ï¸ æœªæ£€æµ‹åˆ° MPSï¼Œå°†ä½¿ç”¨ CPU (ä¼šæ¯”è¾ƒæ…¢)")

# æ–‡ä»¶é…ç½®
DATA_PATH = "cambridge_flan_t5_512.jsonl" 
MODEL_NAME = "google/flan-t5-base"            

MAX_SOURCE_LEN = 512
MAX_TARGET_LEN = 512

# =====================================
# 2. è®­ç»ƒé€»è¾‘
# =====================================

def main():
    print(f"Loading dataset from: {DATA_PATH}")
    
    # 1. è¯»å–æ•°æ®
    df = pd.read_json(DATA_PATH, lines=True)
    print(f"Total examples: {len(df)}")

    # 2. æŒ‰æ–‡ç«  ID åˆ‡åˆ† (ä¸¥é˜²æ•°æ®æ³„æ¼)
    # æå–åŸºç¡€ID (0_A1 -> 0)
    df['base_id'] = df['id'].astype(str).apply(lambda x: x.split('_')[0])
    
    unique_ids = df['base_id'].unique()
    np.random.seed(42)
    np.random.shuffle(unique_ids)
    
    # 90% è®­ç»ƒï¼Œ10% éªŒè¯
    split_idx = int(len(unique_ids) * 0.9)
    train_ids = unique_ids[:split_idx]
    val_ids = unique_ids[split_idx:]
    
    train_df = df[df['base_id'].isin(train_ids)]
    val_df = df[df['base_id'].isin(val_ids)]
    
    print(f"Train size: {len(train_df)} | Val size: {len(val_df)}")
    
    train_ds = Dataset.from_pandas(train_df)
    val_ds = Dataset.from_pandas(val_df)

    # 3. Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    def tokenize_fn(batch):
        model_inputs = tokenizer(
            batch["input_text"],
            max_length=MAX_SOURCE_LEN,
            truncation=True,
            padding=False, 
        )
        labels = tokenizer(
            text_target=batch["target_text"], 
            max_length=MAX_TARGET_LEN,
            truncation=True,
            padding=False,
        )
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    print("Tokenizing datasets...")
    # remove_columns å¾ˆé‡è¦ï¼Œé˜²æ­¢æ ¼å¼å†²çª
    train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)
    val_tok = val_ds.map(tokenize_fn, batched=True, remove_columns=val_ds.column_names)

    # 4. åŠ è½½æ¨¡å‹
    print(f"Loading Model: {MODEL_NAME}")
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
    model.to(DEVICE) # ç§»åŠ¨åˆ° GPU

    # 5. é…ç½® LoRA (è½»é‡åŒ–å¾®è°ƒ)
    # è¿™è®©ä½ çš„ Mac å³ä½¿è·‘å¤§ä¸€ç‚¹çš„æ¨¡å‹ä¹Ÿä¸ä¼šå‘çƒ«å¤ªä¸¥é‡
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q", "v"], # T5 çš„æ³¨æ„åŠ›å±‚
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM,
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

# 6. è®­ç»ƒå‚æ•° (Mac ä¼˜åŒ–æœ€ç»ˆç‰ˆ)
    training_args = Seq2SeqTrainingArguments(
        output_dir="mac_flan_t5_finetuned",
        num_train_epochs=5,             
        per_device_train_batch_size=8,  
        per_device_eval_batch_size=8,
        gradient_accumulation_steps=4,  
        learning_rate=3e-4,
        warmup_ratio=0.05,
        
        # Mac å…³é”®è®¾ç½®
        fp16=False, 
        bf16=False, 
        
        logging_steps=10,
        
        # ğŸ”´ å…³é”®ä¿®æ”¹ç‚¹ï¼šè®©ä¿å­˜å’Œè¯„ä¼°é¢‘ç‡ä¸€è‡´ ğŸ”´
        eval_strategy="epoch",  # æ¯è·‘å®Œä¸€è½®ï¼Œè¯„ä¼°ä¸€æ¬¡
        save_strategy="epoch",  # æ¯è·‘å®Œä¸€è½®ï¼Œä¿å­˜ä¸€æ¬¡
        
        save_total_limit=2,     # åªä¿ç•™æœ€å¥½çš„2ä¸ªæ¨¡å‹ï¼Œé˜²æ­¢ç¡¬ç›˜å¡æ»¡
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        
        dataloader_num_workers=0, 
        report_to="none",
    )

    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_tok,
        eval_dataset=val_tok,
        tokenizer=tokenizer,
        data_collator=data_collator,
        callbacks=[],
    )

    print("ğŸš€ Starting local training on Mac M4 Pro...")
    trainer.train()
    
    print("âœ… Training finished.")
    # ä¿å­˜æ¨¡å‹
    save_path = "mac_final_model"
    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)
    print(f"Model saved to ./{save_path}")

if __name__ == "__main__":
    main()